# 日志

1 情感数据分析：以传统语音情感识别数据集及截取文艺作品中的语音数据作为样本数据，通过样本数据训练分类模型来判断语音情感倾向。
2 提取语音特征：通过opensmile或手动提取语音特征
3 建立抑郁情绪评价模型：通过阅读论文，与心理学专家交流等方式建立心理评估模型，以语音分析得到的情感倾向为输入，评价使用者的抑郁情绪状况。

1)对语音信号进行预处理以改善语音质量，本文对输入的语音信号进行预加重和加窗处理，并选取窗长为5的中值滤波器对去噪后的情感语音信号进行平滑处理，并分块成统一大小为 28 28 作为深度学习网络的输入。

2) 采用深度信念网络结构实现情感特征的自动提取。

## 4/7
日尝试将输入所有裁切后的数据送给cnn，结果并没有解决过拟合问题，训练后的正确率反而下降

## 4/12
与学姐交谈，基于attention的双向lstm结构实现的效果貌似不错，下一个方向

## 4/14

为模型增加dropout以及128个节点的Dense层，可以看到过拟合的速度变慢，但是最终的测试集正确率依然在70%上下，说明训练数据可能泛化能力不强

+ [ ] 优化cnn模型，可以尝试SGD，Adagrad，Adadelta，Adam，Adamax，Nadam这些不同优化器的效果

+ [x] 复现论文，concat操作

+ [ ] 考虑正确率的规范性，UAR,WAR？

+ [ ] 学习attention原理，将attention层加入我的模型

+ [ ] 弄清楚lstm模型，将模型迁移到lstm

#### 我的工作

简化整合之前的工作流代码，将Sequential模型改为API构建的模型，添加了频域以及时域的卷积层连接；

查询得history.history调出epoch历史的操作





## 4/26

baseline

```python
from keras import Input,layers,Model
from keras.layers import  Input, Flatten, Dense, Dropout, \
Conv2D, MaxPooling2D, BatchNormalization, Activation, Concatenate,Reshape,Dot,Lambda,GlobalAveragePooling2D
def get_model():
    input_tensor=Input(shape=(128,63,1))
    conv1a=layers.Conv2D(8,(2,8),activation='relu',padding='same')(input_tensor)
    conv1b=layers.Conv2D(8,(8,2),activation='relu',padding='same')(input_tensor)
    conv1 = layers.concatenate([conv1a,conv1b],axis=-1)
    MaxPooling=layers.MaxPooling2D(2,2)(conv1)
    conv2=layers.Conv2D(32,(3,3),activation='relu',padding = 'same')(MaxPooling)
    MaxPooling=layers.MaxPooling2D(2,2)(conv2)
    conv3=layers.Conv2D(128,(3,3),activation='relu',padding = 'same')(MaxPooling)
    MaxPooling=layers.MaxPooling2D(2,2)(conv3)
    MaxPooling=layers.Dropout(0.15)(MaxPooling)
    Flatten=layers.Flatten()(MaxPooling)
    dense=layers.Dense(128,activation='relu')(Flatten)
    dense=layers.Dropout(0.3)(dense)
    output_tensor=layers.Dense(1,activation='sigmoid')(Flatten)
    API_model=Model(input_tensor,output_tensor)
    API_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return API_model
```



今天只有一个目标

+ [x] 将attention层加入到我的模型里

![image-20200426214925474](C:\Users\Forre\AppData\Roaming\Typora\typora-user-images\image-20200426214925474.png)

attention加进去效果没有那么好

baseline

```python 
sgd = optimizers.SGD(lr=0.001, clipnorm=1.)
input_tensor=Input(shape=(128,63,1))
conv1a=layers.Conv2D(8,(2,8),activation='relu',padding='same')(input_tensor)
conv1b=layers.Conv2D(8,(8,2),activation='relu',padding='same')(input_tensor)
conv1 = layers.concatenate([conv1a,conv1b],axis=-1)
conv1=BatchNormalization()(conv1)
MaxPooling=layers.MaxPooling2D(2,2)(conv1)
conv2=layers.Conv2D(32,(3,3),activation='relu',padding = 'same')(MaxPooling)
conv2=BatchNormalization()(conv2)
MaxPooling=layers.MaxPooling2D(2,2)(conv2)
conv3=layers.Conv2D(48,(3,3),activation='relu',padding = 'same')(MaxPooling)
conv3=BatchNormalization()(conv3)
MaxPooling=layers.MaxPooling2D(2,2)(conv3)
conv4=layers.Conv2D(64,(3,3),activation='relu',padding = 'same')(MaxPooling)
conv4=BatchNormalization()(conv4)
MaxPooling=layers.MaxPooling2D(2,2)(conv4)
conv5a=layers.Conv2D(1,(1,1),activation='relu',padding='same')(MaxPooling)
conv5b=layers.Conv2D(1,(1,1),activation='sigmoid',padding='same')(MaxPooling)
Multiply= multiply([conv5a,conv5b])
output_tensor=layers.GlobalAveragePooling2D()(Multiply)
API_model=Model(input_tensor,output_tensor)
API_model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])
```



## 4/28

### AVEC 2016-Depression

["AVEC 2016 – Depression, Mood, and Emotion Recognition,Workshop and Challenge"](https://dl.acm.org/doi/pdf/10.1145/2988257.2988258)

声学的的LLD（low-level-descriptors）包含了频谱、倒谱、韵律、音质信息，从Opensmile kit提取[[参考文献]](F. Eyben, F. Weninger, F. Groß, and B. Schuller.Recent developments in openSMILE, the Munich open-source multimedia feature extractor. In Proc. of ACM MM, pages 835–838, Barcelona, Spain, October 2013)

**grid search**

#### 1. 数据处理：

使用COVAREP(Matlab库)：

* F0,Voicing(VUV)
* Normalized amplitude quotient (NAQ), Quasi open quotient (QOQ), the diﬀerence in amplitude of the ﬁrst two harmonics of the differentiated glottal source spectrum (H1H2), parabolic spectral parameter (PSP), maxima dispersion quotient (MDQ), spectral tilt/slope of wavelet responses (peak-Slope), and shape parameter of the Liljencrants-Fant model of the glottal pulse dynamics (Rd)
* Mel cepstral coeﬃcients (MCEP0-24), Harmonic Model and Phase Distortion mean (HMPDM0-24) and deviations (HMPDD0-12)

#### 2. SVM

scikit-learn toolbox linear-SVM ,optimizers=sgd

采用grid-search寻找最优超参数loss function∈ {logarithmic, hinge loss}, regularization ∈ {L1, L2}, and

α ∈ {1e1, 1e0, . . . , 1e − 5}. ，确定loss function = hinge loss,regularization = L1, and α = 1e − 3. 

可以学习他的量化标准

![image-20200428141744227](C:\Users\Forre\AppData\Roaming\Typora\typora-user-images\image-20200428141744227.png)

#### 3.感想：

可以通过加权虚警惩罚来降低虚警的概率

### AVEC 2019- Depression

[链接](https://dl.acm.org/doi/pdf/10.1145/3347320.3357688)

#### 1.数据处理

使用 extended Geneva Minimalistic

Acoustic Parameter Set (eGeMAPS),包含88维数据，另外增加了openSMILE 获取的MFCC(1-13)及其一阶二阶导数

# 论文草稿

基于情感语音的抑郁情绪评测技术的研究

## 绪论

> 绪论部分主要论述选题的意义、国内外研究现状以及本文主要研究的内容、研究思路以及内容安排等。

语音情感识别旨在通过对语料的分析自动识别人类的情绪。随着诸如智能服务机器人、自动化电话呼叫中心、远程教育等基于语音的人机交互应用快速发展，在过去的三十年里语音情感识别得到了越来越多的关注。

SER系统的一个关键就在于寻找[音素](speech segments)的有效表示。因为情绪表达的复杂性和大数据集的匮乏，要寻找这样的有效表示并非易事。传统的SER方法通常包括基于前端帧的特征提取和用于分类或回归的后端话语表示。Slaney 等人基于梅尔倒谱系数应用高斯混合模型实现了语音情感识别[[1]]( M. Slaney and G. McRoberts, "Baby ears: a recognition system for affective vocalizations," in Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, vol. 2, May 1998, pp. 985–988 vol.2.)。在[[2,3]]( )中，韵律特征被提取出来用于支持向量机模型中。然而这些手动提取的特征对于语音情感信息的特征化可能并不是最优的，这也就是为什么这些模型的性能表现难以让人满意。

受到深度学习技术在语音图像识别上成功的激励[[4,5]]( ) ，人们提出了许多针对SER的深度神经网络和卷积神经网络[6,7, 8, 9, 10, 11, 12]。在[6,7]中，提出了一种多级程序，前端利用DNN和CNN神经网络提取特征，接着在后端利用支持向量机（SVM）和极限学习机（ELM）进行情感分类。[9,11]新近的工作越来越偏向于采用端到端的训练架构。例如，Trigeorgis等人在[9]中直接将原始音频输入给CNN网络实现前端特征提取，接着利用长短期记忆层实现情感表征的学习。其中模型的参数可以通过反向传播算法训练优化。在[8,10]中对时间进行最大池化以从显著性区域获得语料的表征。 Neumann 等人进一步在最大池化后引入注意力机制[12]。而 Mirsamadi 等人将加权池化应用于RNN输出[13]。

尽管最近SER的深度学习方法有所改进，但是一些问题依然存在。首先，时域和频域都包含了语音情感信息。然而如何设计合适的神经网络体系结构来利用时间和频率信息获取有效的情感表示依然不清楚。[14,15]构建二维时频LSTM和Grid-LSTM来对时频的差别建模实现大规模自动语音识别(ASR)。但是复杂的模型架构在诸如IEMOCAP等小数据集中容易出现过拟合的情况[16]。此外对于用于分析复杂情绪表达用到的高阶统计量，仅凭平均池化或者最大池化可能不足以得到有效表示。最近的一些工作展现了学习特征表示的情境下引入注意力机制的好处[12,13,10]，但是它们通常以自下而上的方式从特征中得到显著性区域。

为了解决这些问题，我们提出了一种针对SER的基于注意力池化的特征学习方法，如图1所示。一个深度卷积网络直接应用于从语音采集而来频谱图上，其中设计了两组不同形状的卷积滤波器用捕获时域和频域的上下文信息。3.2节中给出的结果将表明，卷积滤波器的形状可能会影响情绪表示的效果。受GoogLeNet [17]的启发，所学习的特征被进一步级联并馈入以下卷积层。为了有效地学习特征，创造性地提出了一种新型注意力池化方法。与现有地基于注意力的SER方法不同，两种注意力特征图，一个类别无关一个针对特定类别情感组合起来用以更有效地表征情感。第一个注意力图是以自下而上的方式得出的，而第二个则与情感类型直接相关。使用IEMOCAP简易数据集对这种表示方法的有效性进行了评估。结果表明，WA的SER性能为71.8％，UA的SER性能为68.0％，**两者均明显优于现有的最新方法**。

**References**
[1] M. Slaney and G. McRoberts, “Baby ears: a recognition system for affective vocalizations,” in Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, vol. 2, May 1998, pp. 985–988 vol.2.
[2] F. Yu, E. Chang, Y.-Q. Xu, and H.-Y. Shum, “Emotion detection from speech to enrich multimedia content,” Advances in multimedia information processing, pp. 550–557, 2001.
[3] I. Luengo, E. Navas, I. Hern´aez, and J. S´anchez, “Automatic emotion recognition using prosodic parameters,” in Proc. of INTERSPEECH. Citeseer, 2005.
[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in International Conference on Neural Information Processing Systems, 2012, pp. 1097–1105.
[5] L. Deng and D. Yu, “Deep learning: Methods and applications,” Foundations & Trends in Signal Processing, vol. 7, no. 3, pp. 197– 387, 2014.
[6] K. Han, D. Yu, and I. Tashev, “Speech Emotion Recognition using Deep Neural Network and Extreme Learning Machine,” in Proceedings of Interspeech, no. September, 2014, pp. 223–227.
[7] Q. Mao, M. Dong, Z. Huang, and Y. Zhan, “Learning salient features for speech emotion recognition using convolutional neural networks,” IEEE Transactions on Multimedia, vol. 16, no. 8, pp. 2203–2213, 2014.
[8] D. Bertero and P. Fung, “A First Look into A Convolutional Neural Network for Speech Emotion Detection,” in IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), 2017, pp. 5115–5119.
[9] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, “Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5200–5204.
[10] Z. Aldeneh and E. M. Provost, “Using Regional Saliency for Speech Emotion Recognition,” in ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, no. 3, 2017, pp. 2741–2745.
[11] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recognition from speech using deep learning on spectrograms,” in Proc. of INTERSPEECH, 2017, pp. 1089–1093.
[12] M. Neumann and N. T. Vu, “Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech,” in Proc. of INTERSPEECH, 2017, pp. 1263–1267.
[13] S. Mirsamadi, E. Barsoum, and C. Zhang, “Automatic speech emotion recognition using recurrent neural networks with local attention,” in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 2227– 2231.
[14] J. Li, A. Mohamed, G. Zweig, and Y. Gong, “Exploring multidimensional lstms for large vocabulary ASR,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2016, pp. 4940–4944.
[15] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, I. Shafran, H. Sak, G. Pundak, and K. Chin, “Acoustic modeling for Google home,” in INTERSPEECH, 2017, pp. 399– 403.
[16] C. Busso, M. Bulut, C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: interactive emotional dyadic motion capture database,” Language Resources and Evaluation, vol. 42, no. 4, p. 335, 2008.
[17] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015, pp. 1–9.

[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016, pp. 770–778.
[19] R. Girdhar and D. Ramanan, “Attentional pooling for action recognition,” ArXiv e-prints, Nov. 2017. [Online]. Available: http://arxiv.org/abs/1711.01467
[20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear CNN models for ﬁne-grained visual recognition,” in International Conference on Computer Vision (ICCV), 2015.
[21] ——, “Bilinear CNNs for ﬁne-grained visual recognition,” in Transactions of Pattern Analysis and Machine Intelligence (PAMI), 2017.
[22] V. Navalpakkam and L. Itti, “An integrated model of top-down and bottom-up attention for optimizing detection speed,” in 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2, 2006, pp. 2049–2056.
[23] R. Xia and Y. Liu, “DBN-ivector Framework for Acoustic Emotion Recognition,” in Proceedings of Interspeech, 2016, pp. 480– 484.
[24] J. Lee and I. Tashev, “High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition,” in Interspeech, 2015, pp. 1537–1540.
[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” in NIPS Workshop, 2017.
[26] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” pp. 448– 456, 2015.

摘录：[AVEC-2019]

抑郁症，特别是重度抑郁症（MDD），是一种常见的心理健康问题，会对人们的思维，感觉和行为产生负面影响[3]。 它可能导致各种情感和身体问题，并影响工作和个人生活的许多方面。 世界卫生组织（WHO）宣布抑郁症是2015年全球疾病和残障的主要原因：超过3亿人生活在其中[40]。 鉴于抑郁症的高患病率和自杀风险，寻找新的诊断和治疗方法变得越来越关键。

证据表明，抑郁和相关的心理健康疾病往往都伴随着行为模式的改变。基于此，人们对于通过分析诸如面部表情、语音韵律等表现上的线索来实现计算机自动诊断的兴趣越来越浓厚。面部活动，手势，头部运动和表情都与抑郁症密切相关。

早期对抑郁症的语言进行的语言学调查发现，患者一致的表现出韵律性语音异常，例如音调降低，音调范围减小，语速降低和发音错误率较高[11]。





## 特征

基于语音信号进行情感识别通常依赖于专门的特征集，这些特征集的提取是基于语音处理领域数十年研究中获得的专业知识。例如MFCC。



## 实验数据（重要）

[DAIC官网](http://dcapswoz.ict.usc.edu)